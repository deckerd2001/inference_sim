{
  "model": "llama2-70b",
  "cluster": {
    "xpu": "a100-80gb",
    "n_xpus_per_node": 8,
    "n_nodes": 1
  },
  "parallelism": {
    "tensor_parallel_size": 8
  },
  "workload": {
    "avg_input_length": 1024,
    "max_input_length": 2048,
    "avg_output_length": 256,
    "max_output_length": 512,
    "arrival_rate": 10.0
  },
  "scheduler": {
    "batching_strategy": "greedy",
    "max_batch_size": null
  },
  "simulation_duration_s": 300.0,
  "random_seed": 42
}
