=======================================================================
LLM Inference Simulator - TP Scaling Benchmark
=======================================================================
Start Time: 2025. 12. 08. (Ïõî) 14:18:43 PST
Random Seed: 22767

‚ö†Ô∏è  Note: Only TP (Tensor Parallelism) is implemented

Configuration:
  Model:           llama2-70b
  xPUs:            a100-80gb h100-80gb mi300x
  Arrival Rate:    10.0 req/s
  Duration:        60.0s

Testing 3 xPUs √ó 4 TP configs = 12 total tests
=======================================================================

-----------------------------------------------------------------------
[1/12] Testing: a100-80gb - TP=1
  Cluster: 1 xPUs (TP=1)
-----------------------------------------------------------------------
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/cli.py", line 219, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/cli.py", line 203, in main
    config = create_config_from_args(args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/cli.py", line 83, in create_config_from_args
    return SimulatorConfig(
           ^^^^^^^^^^^^^^^^
  File "<string>", line 12, in __init__
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/config.py", line 113, in __post_init__
    self.validate()
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/config.py", line 212, in validate
    raise ValueError(error_msg.strip())
ValueError: Configuration validation failed:
  1. Model weights (130.39GB per xPU with TP=1) exceed xPU memory (80.0GB). Increase tensor_parallel_size or use larger xPUs.
‚úó Failed
  Error: 1. Model weights (130.39GB per xPU with TP=1) exceed xPU memory (80.0GB). Increase tensor_parallel_size or use larger xPUs.

-----------------------------------------------------------------------
[2/12] Testing: a100-80gb - TP=2
  Cluster: 2 xPUs (TP=2)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 2 xPUs, arrival_rate=10.0 req/s
Memory: 65.19GB model, 14.81GB available for KV cache

Simulation completed at t=60.03s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 61

Throughput:
  Requests/sec: 1.02
  Tokens/sec: 128.51

xPU Utilization: 100.0%

Memory Usage:
  Peak:        80.00GB / 80GB (100.0%)
  P95:         79.87GB / 80GB ( 99.8%)
  P50 (Med):   75.69GB / 80GB ( 94.6%)

First Token Latency (seconds):
  Mean: 25.4699
  P50:  24.1105
  P90:  47.9262
  P95:  48.1342
  P99:  48.3051

End-to-End Latency (seconds):
  Mean: 29.5248
  P50:  28.6239
  P90:  50.9477
  P95:  53.3239
  P99:  54.0922

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/a100-80gb_2xpu_tp2.json
‚úì Completed successfully
  Throughput: 128.5 tok/s, Completed: 61, P95 TTFT: 48.13s

-----------------------------------------------------------------------
[3/12] Testing: a100-80gb - TP=4
  Cluster: 4 xPUs (TP=4)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 4 xPUs, arrival_rate=10.0 req/s
Memory: 32.60GB model, 47.40GB available for KV cache

Simulation completed at t=60.02s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 172

Throughput:
  Requests/sec: 2.87
  Tokens/sec: 361.65

xPU Utilization: 100.0%

Memory Usage:
  Peak:        80.00GB / 80GB (100.0%)
  P95:         79.56GB / 80GB ( 99.5%)
  P50 (Med):   62.53GB / 80GB ( 78.2%)

First Token Latency (seconds):
  Mean: 23.3758
  P50:  23.4771
  P90:  42.7538
  P95:  43.6926
  P99:  44.0079

End-to-End Latency (seconds):
  Mean: 24.4524
  P50:  26.4493
  P90:  40.3674
  P95:  41.6421
  P99:  43.3111

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/a100-80gb_4xpu_tp4.json
‚úì Completed successfully
  Throughput: 361.6 tok/s, Completed: 172, P95 TTFT: 43.69s

-----------------------------------------------------------------------
[4/12] Testing: a100-80gb - TP=8
  Cluster: 8 xPUs (TP=8)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 8 xPUs, arrival_rate=10.0 req/s
Memory: 16.30GB model, 63.70GB available for KV cache

Simulation completed at t=60.01s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 253

Throughput:
  Requests/sec: 4.22
  Tokens/sec: 560.34

xPU Utilization: 100.0%

Memory Usage:
  Peak:        79.72GB / 80GB ( 99.7%)
  P95:         79.31GB / 80GB ( 99.1%)
  P50 (Med):   59.32GB / 80GB ( 74.1%)

First Token Latency (seconds):
  Mean: 18.2009
  P50:  17.7657
  P90:  31.9860
  P95:  33.6351
  P99:  34.8601

End-to-End Latency (seconds):
  Mean: 20.1529
  P50:  20.6112
  P90:  32.9049
  P95:  34.0810
  P99:  35.5059

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/a100-80gb_8xpu_tp8.json
‚úì Completed successfully
  Throughput: 560.3 tok/s, Completed: 253, P95 TTFT: 33.64s

-----------------------------------------------------------------------
[5/12] Testing: h100-80gb - TP=1
  Cluster: 1 xPUs (TP=1)
-----------------------------------------------------------------------
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/cli.py", line 219, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/cli.py", line 203, in main
    config = create_config_from_args(args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/cli.py", line 83, in create_config_from_args
    return SimulatorConfig(
           ^^^^^^^^^^^^^^^^
  File "<string>", line 12, in __init__
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/config.py", line 113, in __post_init__
    self.validate()
  File "/home/deckerd/my_projects/inference_sim/llm_inference_simulator/config.py", line 212, in validate
    raise ValueError(error_msg.strip())
ValueError: Configuration validation failed:
  1. Model weights (130.39GB per xPU with TP=1) exceed xPU memory (80.0GB). Increase tensor_parallel_size or use larger xPUs.
‚úó Failed
  Error: 1. Model weights (130.39GB per xPU with TP=1) exceed xPU memory (80.0GB). Increase tensor_parallel_size or use larger xPUs.

-----------------------------------------------------------------------
[6/12] Testing: h100-80gb - TP=2
  Cluster: 2 xPUs (TP=2)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 2 xPUs, arrival_rate=10.0 req/s
Memory: 65.19GB model, 14.81GB available for KV cache

Simulation completed at t=60.01s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 106

Throughput:
  Requests/sec: 1.77
  Tokens/sec: 221.86

xPU Utilization: 100.0%

Memory Usage:
  Peak:        80.00GB / 80GB (100.0%)
  P95:         79.84GB / 80GB ( 99.8%)
  P50 (Med):   75.22GB / 80GB ( 94.0%)

First Token Latency (seconds):
  Mean: 25.8045
  P50:  25.2958
  P90:  47.1784
  P95:  50.8206
  P99:  51.1078

End-to-End Latency (seconds):
  Mean: 26.4163
  P50:  26.1629
  P90:  46.3361
  P95:  49.6668
  P99:  50.6814

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/h100-80gb_2xpu_tp2.json
‚úì Completed successfully
  Throughput: 221.9 tok/s, Completed: 106, P95 TTFT: 50.82s

-----------------------------------------------------------------------
[7/12] Testing: h100-80gb - TP=4
  Cluster: 4 xPUs (TP=4)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 4 xPUs, arrival_rate=10.0 req/s
Memory: 32.60GB model, 47.40GB available for KV cache

Simulation completed at t=60.01s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 324

Throughput:
  Requests/sec: 5.40
  Tokens/sec: 688.34

xPU Utilization: 100.0%

Memory Usage:
  Peak:        79.71GB / 80GB ( 99.6%)
  P95:         79.58GB / 80GB ( 99.5%)
  P50 (Med):   61.33GB / 80GB ( 76.7%)

First Token Latency (seconds):
  Mean: 15.2294
  P50:  15.5536
  P90:  25.9708
  P95:  27.9520
  P99:  28.9859

End-to-End Latency (seconds):
  Mean: 17.3555
  P50:  17.2843
  P90:  27.9354
  P95:  29.3863
  P99:  30.7795

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/h100-80gb_4xpu_tp4.json
‚úì Completed successfully
  Throughput: 688.3 tok/s, Completed: 324, P95 TTFT: 27.95s

-----------------------------------------------------------------------
[8/12] Testing: h100-80gb - TP=8
  Cluster: 8 xPUs (TP=8)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 8 xPUs, arrival_rate=10.0 req/s
Memory: 16.30GB model, 63.70GB available for KV cache

Simulation completed at t=60.02s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 466

Throughput:
  Requests/sec: 7.76
  Tokens/sec: 1005.62

xPU Utilization: 100.0%

Memory Usage:
  Peak:        79.60GB / 80GB ( 99.5%)
  P95:         79.04GB / 80GB ( 98.8%)
  P50 (Med):   52.55GB / 80GB ( 65.7%)

First Token Latency (seconds):
  Mean: 8.0515
  P50:  8.1133
  P90:  13.7278
  P95:  14.7651
  P99:  16.2252

End-to-End Latency (seconds):
  Mean: 10.1731
  P50:  10.5319
  P90:  15.5430
  P95:  16.7747
  P99:  18.1694

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/h100-80gb_8xpu_tp8.json
‚úì Completed successfully
  Throughput: 1005.6 tok/s, Completed: 466, P95 TTFT: 14.77s

-----------------------------------------------------------------------
[9/12] Testing: mi300x - TP=1
  Cluster: 1 xPUs (TP=1)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 1 xPUs, arrival_rate=10.0 req/s
Memory: 130.39GB model, 61.61GB available for KV cache

Simulation completed at t=62.18s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 215

Throughput:
  Requests/sec: 3.46
  Tokens/sec: 433.18

xPU Utilization: 100.0%

Memory Usage:
  Peak:       192.00GB / 192GB (100.0%)
  P95:        191.59GB / 192GB ( 99.8%)
  P50 (Med):  170.38GB / 192GB ( 88.7%)

First Token Latency (seconds):
  Mean: 20.4797
  P50:  22.2582
  P90:  35.1798
  P95:  36.4089
  P99:  37.2658

End-to-End Latency (seconds):
  Mean: 24.4318
  P50:  24.6673
  P90:  39.2558
  P95:  40.4630
  P99:  42.6009

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/mi300x_1xpu_tp1.json
‚úì Completed successfully
  Throughput: 433.2 tok/s, Completed: 215, P95 TTFT: 36.41s

-----------------------------------------------------------------------
[10/12] Testing: mi300x - TP=2
  Cluster: 2 xPUs (TP=2)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 2 xPUs, arrival_rate=10.0 req/s
Memory: 65.19GB model, 126.81GB available for KV cache

Simulation completed at t=62.59s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 453

Throughput:
  Requests/sec: 7.24
  Tokens/sec: 899.32

xPU Utilization: 100.0%

Memory Usage:
  Peak:       191.97GB / 192GB (100.0%)
  P95:        191.28GB / 192GB ( 99.6%)
  P50 (Med):  135.04GB / 192GB ( 70.3%)

First Token Latency (seconds):
  Mean: 10.1114
  P50:  9.8943
  P90:  16.0833
  P95:  16.7097
  P99:  18.3413

End-to-End Latency (seconds):
  Mean: 13.9527
  P50:  14.0514
  P90:  20.2037
  P95:  21.6274
  P99:  23.4865

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/mi300x_2xpu_tp2.json
‚úì Completed successfully
  Throughput: 899.3 tok/s, Completed: 453, P95 TTFT: 16.71s

-----------------------------------------------------------------------
[11/12] Testing: mi300x - TP=4
  Cluster: 4 xPUs (TP=4)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 4 xPUs, arrival_rate=10.0 req/s
Memory: 32.60GB model, 159.40GB available for KV cache

Simulation completed at t=60.25s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 584

Throughput:
  Requests/sec: 9.69
  Tokens/sec: 1217.26

xPU Utilization: 100.0%

Memory Usage:
  Peak:       137.23GB / 192GB ( 71.5%)
  P95:        131.46GB / 192GB ( 68.5%)
  P50 (Med):   78.42GB / 192GB ( 40.8%)

First Token Latency (seconds):
  Mean: 3.7474
  P50:  3.5912
  P90:  6.0419
  P95:  6.6177
  P99:  7.4084

End-to-End Latency (seconds):
  Mean: 6.3866
  P50:  6.3944
  P90:  9.1410
  P95:  9.7062
  P99:  10.5995

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/mi300x_4xpu_tp4.json
‚úì Completed successfully
  Throughput: 1217.3 tok/s, Completed: 584, P95 TTFT: 6.62s

-----------------------------------------------------------------------
[12/12] Testing: mi300x - TP=8
  Cluster: 8 xPUs (TP=8)
-----------------------------------------------------------------------

Starting simulation...
Starting simulation...
Configuration: LLaMA-2-70B, 8 xPUs, arrival_rate=10.0 req/s
Memory: 16.30GB model, 175.70GB available for KV cache

Simulation completed at t=60.01s

============================================================
SIMULATION SUMMARY
============================================================

Requests:
  Total: 650
  Completed: 624

Throughput:
  Requests/sec: 10.40
  Tokens/sec: 1338.63

xPU Utilization: 100.0%

Memory Usage:
  Peak:        71.24GB / 192GB ( 37.1%)
  P95:         63.42GB / 192GB ( 33.0%)
  P50 (Med):   40.62GB / 192GB ( 21.2%)

First Token Latency (seconds):
  Mean: 1.6155
  P50:  1.6205
  P90:  2.6156
  P95:  2.7948
  P99:  3.1011

End-to-End Latency (seconds):
  Mean: 2.9437
  P50:  2.9620
  P90:  4.1374
  P95:  4.4028
  P99:  4.8239

============================================================

‚úì Results saved to: results/cluster_benchmark_20251208_141843/mi300x_8xpu_tp8.json
‚úì Completed successfully
  Throughput: 1338.6 tok/s, Completed: 624, P95 TTFT: 2.79s

=======================================================================
Benchmark Complete!
=======================================================================
End Time: 2025. 12. 08. (Ïõî) 14:18:50 PST

=============================================================================================================================
TP SCALING BENCHMARK REPORT
=============================================================================================================================

Configuration:
-----------------------------------------------------------------------------------------------------------------------------
  Model:           N/A
  xPUs Tested:     N/A
  Workload:        N/A req/s
  Duration:        N/As

Performance & Cost Analysis:
=============================================================================================================================
         xPU  GPUs  TP   Status  Throughput  P95 TTFT     Cost  Efficiency        Cost
                                    (tok/s)     (sec)   ($/hr)   (tok/$/h)   ($/1Mtok)
-----------------------------------------------------------------------------------------------------------------------------
   A100-80GB     1   1   ‚ùå FAIL         N/A       N/A      N/A         N/A         N/A
   A100-80GB     2   2     ‚úÖ OK       128.5     48.13     7.34        17.5    57116.36
   A100-80GB     4   4     ‚úÖ OK       361.6     43.69    14.68        24.6    40592.28
   A100-80GB     8   8     ‚úÖ OK       560.3     33.64    29.36        19.1    52396.38
   H100-80GB     1   1   ‚ùå FAIL         N/A       N/A      N/A         N/A         N/A
   H100-80GB     2   2     ‚úÖ OK       221.9     50.82    12.98        17.1    58505.86
   H100-80GB     4   4     ‚úÖ OK       688.3     27.95    25.96        26.5    37714.11
   H100-80GB     8   8     ‚úÖ OK      1005.6     14.77    51.92        19.4    51629.93
      MI300X     1   1     ‚úÖ OK       433.2     36.41     7.00        61.9    16159.42
      MI300X     2   2     ‚úÖ OK       899.3     16.71    14.00        64.2    15567.35
      MI300X     4   4     ‚úÖ OK      1217.3      6.62    28.00        43.5    23002.53
      MI300X     8   8     ‚úÖ OK      1338.6      2.79    56.00        23.9    41833.73
-----------------------------------------------------------------------------------------------------------------------------

üèÜ Recommended Configurations:
-----------------------------------------------------------------------------------------------------------------------------
  üí∞ Best Value (tok/$/hour):
     MI300X: 2 GPUs, TP=2
     64.2 tok/$/hour | $15567.35/1M tokens | 899.3 tok/s

  üöÄ Best Performance (throughput):
     MI300X: 8 GPUs, TP=8
     1338.6 tok/s | $56.00/hour | P95 TTFT: 2.79s

  ‚ö° Best Latency (TTFT):
     MI300X: 8 GPUs, TP=8
     P95 TTFT: 2.79s | 1338.6 tok/s | $56.00/hour


‚ùå Failed Tests:
-----------------------------------------------------------------------------------------------------------------------------
  A100-80GB: 1 GPUs, TP=1
    ‚Üí 1. Model weights (130.39GB per xPU with TP=1) exceed xPU memory (80.0GB). Increase tensor_parallel_size or use larger xPUs.
  H100-80GB: 1 GPUs, TP=1
    ‚Üí 1. Model weights (130.39GB per xPU with TP=1) exceed xPU memory (80.0GB). Increase tensor_parallel_size or use larger xPUs.


üìà TP Scaling Efficiency:
-----------------------------------------------------------------------------------------------------------------------------
  A100-80GB (baseline: 2 GPU, TP=2, 128.5 tok/s):
    4 GPUs, TP=4: 2.81x speedup (140.7% efficiency, ideal: 2.0x)
    8 GPUs, TP=8: 4.36x speedup (109.0% efficiency, ideal: 4.0x)

  H100-80GB (baseline: 2 GPU, TP=2, 221.9 tok/s):
    4 GPUs, TP=4: 3.10x speedup (155.1% efficiency, ideal: 2.0x)
    8 GPUs, TP=8: 4.53x speedup (113.3% efficiency, ideal: 4.0x)

  MI300X (baseline: 1 GPU, TP=1, 433.2 tok/s):
    2 GPUs, TP=2: 2.08x speedup (103.8% efficiency, ideal: 2.0x)
    4 GPUs, TP=4: 2.81x speedup (70.3% efficiency, ideal: 4.0x)
    8 GPUs, TP=8: 3.09x speedup (38.6% efficiency, ideal: 8.0x)

=============================================================================================================================
Summary: 10 succeeded, 2 failed
=============================================================================================================================

Results saved to: results/cluster_benchmark_20251208_141843
=======================================================================
