# LLM Inference Simulator - í€µìŠ¤íƒ€íŠ¸ ê°€ì´ë“œ

## ì„¤ì¹˜

```bash
# ì˜ì¡´ì„± ì„¤ì¹˜
pip install numpy

# íŒ¨í‚¤ì§€ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd llm_inference_simulator
```

## 5ë¶„ íŠœí† ë¦¬ì–¼

### 1. ê¸°ë³¸ ì‹œë®¬ë ˆì´ì…˜

ê°€ì¥ ê°„ë‹¨í•œ ì˜ˆì œë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤:

```python
from llm_inference_simulator import *

# ì„¤ì • ìƒì„±
config = SimulatorConfig(
    model_spec=ModelSpec(
        name="llama-7b",
        n_params=7_000_000_000,
        hidden_size=4096,
        n_layers=32,
        n_heads=32,
        ffn_dim=11008,
    ),
    workload_spec=WorkloadSpec(
        avg_input_length=512,
        avg_output_length=128,
        arrival_rate=2.0,  # 2 requests/sec
    ),
    cluster_spec=ClusterSpec(
        n_gpus_per_node=1,
        n_nodes=1,
        gpu_spec=GPUSpec(name="A100-80GB"),
    ),
    simulation_duration_s=60.0,
)

# ì‹¤í–‰
simulator = LLMInferenceSimulator(config)
metrics = simulator.run()

# ê²°ê³¼ í™•ì¸
stats = metrics.compute_statistics()
print(f"Throughput: {stats['throughput_tokens_per_sec']:.2f} tokens/sec")
```

### 2. ì„±ëŠ¥ ë¹„êµ

ë‘ ê°€ì§€ ì„¤ì •ì„ ë¹„êµí•˜ë ¤ë©´:

```python
# ì„¤ì • 1: Single GPU
config1 = SimulatorConfig(...)
simulator1 = LLMInferenceSimulator(config1)
metrics1 = simulator1.run()

# ì„¤ì • 2: TP=4
config2 = config1
config2.parallelism_spec = ParallelismSpec(tensor_parallel_size=4)
config2.cluster_spec.n_gpus_per_node = 4
simulator2 = LLMInferenceSimulator(config2)
metrics2 = simulator2.run()

# ë¹„êµ
stats1 = metrics1.compute_statistics()
stats2 = metrics2.compute_statistics()
print(f"Speedup: {stats2['throughput_tokens_per_sec'] / stats1['throughput_tokens_per_sec']:.2f}x")
```

### 3. ë‹¤ì–‘í•œ ì›Œí¬ë¡œë“œ í…ŒìŠ¤íŠ¸

ë¶€í•˜ê°€ ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í…ŒìŠ¤íŠ¸:

```python
for arrival_rate in [1, 5, 10, 20]:
    config.workload_spec.arrival_rate = arrival_rate
    
    simulator = LLMInferenceSimulator(config)
    metrics = simulator.run()
    stats = metrics.compute_statistics()
    
    print(f"Load: {arrival_rate} req/s")
    print(f"  Throughput: {stats['throughput_tokens_per_sec']:.2f} tokens/sec")
    print(f"  P95 Latency: {stats['first_token_latency']['p95']:.4f}s")
    print()
```

## ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ëª…

### ModelSpec
- `n_params`: ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ (7B, 13B, 70B ë“±)
- `hidden_size`: Hidden dimension (ë³´í†µ 4096, 8192 ë“±)
- `n_layers`: Transformer layer ìˆ˜
- `n_heads`: Attention head ìˆ˜
- `ffn_dim`: FFN dimension (ë³´í†µ 4 Ã— hidden_size)

### WorkloadSpec
- `avg_input_length`: í‰ê·  ì…ë ¥ í† í° ìˆ˜
- `avg_output_length`: í‰ê·  ì¶œë ¥ í† í° ìˆ˜
- `arrival_rate`: ì´ˆë‹¹ ìš”ì²­ ìˆ˜
- `arrival_process`: "poisson" ë˜ëŠ” "deterministic"

### GPUSpec
- `name`: GPU ì´ë¦„ (í‘œì‹œìš©)
- `compute_tflops`: BF16/FP16 TFLOPS
- `memory_size_gb`: GPU ë©”ëª¨ë¦¬ í¬ê¸°
- `memory_bandwidth_gbs`: HBM ëŒ€ì—­í­

### ParallelismSpec
- `tensor_parallel_size`: Tensor parallelism degree
- `data_parallel_size`: Data parallelism degree
- `pipeline_parallel_size`: Pipeline parallelism degree

### SchedulerSpec
- `batching_type`: "static", "dynamic", "continuous"
- `max_batch_size`: ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
- `token_level_scheduling`: í† í° ë‹¨ìœ„ ìŠ¤ì¼€ì¤„ë§ ì—¬ë¶€

## ì¼ë°˜ì ì¸ ì‚¬ìš© ì‚¬ë¡€

### ì‚¬ë¡€ 1: GPU ì„ íƒ

ì–´ë–¤ GPUë¥¼ ì‚¬ìš©í• ì§€ ê²°ì •:

```python
gpus = [
    ("A100-80GB", 312.0, 80.0, 2039.0),
    ("H100-80GB", 989.0, 80.0, 3350.0),
    ("A100-40GB", 312.0, 40.0, 1555.0),
]

for name, tflops, mem, bw in gpus:
    config.cluster_spec.gpu_spec = GPUSpec(
        name=name,
        compute_tflops=tflops,
        memory_size_gb=mem,
        memory_bandwidth_gbs=bw,
    )
    
    metrics = LLMInferenceSimulator(config).run()
    stats = metrics.compute_statistics()
    
    print(f"{name}: {stats['throughput_tokens_per_sec']:.2f} tokens/sec")
```

### ì‚¬ë¡€ 2: ë°°ì¹˜ í¬ê¸° ìµœì í™”

ìµœì ì˜ ë°°ì¹˜ í¬ê¸° ì°¾ê¸°:

```python
for batch_size in [1, 2, 4, 8, 16, 32]:
    config.scheduler_spec.max_batch_size = batch_size
    config.workload_spec.batch_size = batch_size
    
    metrics = LLMInferenceSimulator(config).run()
    stats = metrics.compute_statistics()
    
    print(f"Batch {batch_size}: "
          f"{stats['throughput_tokens_per_sec']:.2f} tok/s, "
          f"P95: {stats['first_token_latency']['p95']:.4f}s")
```

### ì‚¬ë¡€ 3: í…ì„œ ë³‘ë ¬í™” ìŠ¤ì¼€ì¼ë§

TP ìŠ¤ì¼€ì¼ë§ íš¨ìœ¨ì„± ì¸¡ì •:

```python
for tp_size in [1, 2, 4, 8]:
    config.parallelism_spec.tensor_parallel_size = tp_size
    config.cluster_spec.n_gpus_per_node = tp_size
    
    metrics = LLMInferenceSimulator(config).run()
    stats = metrics.compute_statistics()
    
    print(f"TP={tp_size}: {stats['throughput_tokens_per_sec']:.2f} tokens/sec")
```

### ì‚¬ë¡€ 4: ì›Œí¬ë¡œë“œ íŒ¨í„´ ë¶„ì„

ë‹¤ì–‘í•œ ì…ì¶œë ¥ ê¸¸ì´ ì¡°í•©:

```python
for input_len, output_len in [(128, 32), (512, 128), (2048, 512)]:
    config.workload_spec.avg_input_length = input_len
    config.workload_spec.avg_output_length = output_len
    
    metrics = LLMInferenceSimulator(config).run()
    stats = metrics.compute_statistics()
    
    print(f"I/O: {input_len}/{output_len}")
    print(f"  Throughput: {stats['throughput_tokens_per_sec']:.2f}")
    print(f"  P95 TTFT: {stats['first_token_latency']['p95']:.4f}s")
```

## ë©”íŠ¸ë¦­ í•´ì„

### First Token Latency (TTFT)
- ì‚¬ìš©ìê°€ ì²« ì‘ë‹µì„ ë°›ê¸°ê¹Œì§€ì˜ ì‹œê°„
- Prefill ì„±ëŠ¥ì— ì˜ì¡´
- ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (ë³´í†µ ìˆ˜ì‹­ ms ~ ìˆ˜ë°± ms)

### End-to-End Latency
- ì „ì²´ ì‘ë‹µ ì™„ë£Œê¹Œì§€ì˜ ì‹œê°„
- TTFT + ë””ì½”ë”© ì‹œê°„
- ê¸´ ì‘ë‹µì¼ìˆ˜ë¡ ì¦ê°€

### Throughput
- ì‹œìŠ¤í…œì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì´ˆë‹¹ í† í° ìˆ˜
- ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
- Batch sizeì™€ GPU ì„±ëŠ¥ì— ì˜ì¡´

### GPU Utilization
- GPUê°€ ì‹¤ì œë¡œ ì‘ì—…í•œ ì‹œê°„ ë¹„ìœ¨
- 100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ íš¨ìœ¨ì 
- ë‚®ìœ¼ë©´ ë°°ì¹˜ í¬ê¸° ì¦ê°€ ê³ ë ¤

## ë¬¸ì œ í•´ê²°

### Q: Completed requestsê°€ 0ì…ë‹ˆë‹¤
A: ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ì´ ì§§ì•„ì„œ ìš”ì²­ì´ ì™„ë£Œë˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
   `simulation_duration_s`ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ `avg_output_length`ë¥¼ ì¤„ì´ì„¸ìš”.

### Q: GPU Utilizationì´ ë‚®ìŠµë‹ˆë‹¤
A: ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ arrival rateë¥¼ ë†’ì´ì„¸ìš”.

### Q: Latencyê°€ ë„ˆë¬´ ë†’ìŠµë‹ˆë‹¤
A: ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜, ë” ë¹ ë¥¸ GPUë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜,
   TPë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”.

### Q: ë©”ëª¨ë¦¬ ë¶€ì¡± ê²½ê³ ê°€ ë‚˜ì˜µë‹ˆë‹¤
A: ëª¨ë¸ í¬ê¸°ì™€ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜,
   ë©”ëª¨ë¦¬ê°€ ë” í° GPUë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

## ë‹¤ìŒ ë‹¨ê³„

1. **ì˜ˆì œ ì‹¤í–‰**: `python example.py`ë¡œ ë‹¤ì–‘í•œ ì˜ˆì œ í™•ì¸
2. **ë¬¸ì„œ ì½ê¸°**: `README.md`ì™€ `ARCHITECTURE.md` ì°¸ê³ 
3. **ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ìì‹ ì˜ ëª¨ë¸ê³¼ ì›Œí¬ë¡œë“œë¡œ í…ŒìŠ¤íŠ¸
4. **í™•ì¥**: ìƒˆë¡œìš´ ìŠ¤ì¼€ì¤„ë§ ì •ì±…ì´ë‚˜ ì„±ëŠ¥ ëª¨ë¸ ì¶”ê°€

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- **ì˜ˆì œ ì½”ë“œ**: `example.py`
- **ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸**: `simple_test.py`
- **ì•„í‚¤í…ì²˜ ë¬¸ì„œ**: `ARCHITECTURE.md`
- **ì „ì²´ ë¬¸ì„œ**: `README.md`

ì¦ê±°ìš´ ì‹œë®¬ë ˆì´ì…˜ ë˜ì„¸ìš”! ğŸš€
